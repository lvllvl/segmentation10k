{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seg10k_050421.ipynb",
      "provenance": [],
      "mount_file_id": "1xftY0TNcgG-DG6eCVkMM3xxvXOyIOC3U",
      "authorship_tag": "ABX9TyPr+8xx0okDlXpBYW8zf+IW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lvllvl/segmentation10k/blob/main/Seg10k_050421.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ6bjbTPTedL"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ENDDWe9_fca"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7WwEtdvPAwK"
      },
      "source": [
        "import os \n",
        "from pathlib import Path \n",
        "import pandas as pd \n",
        "from torchvision.io import read_image\n",
        "\n",
        "import torch \n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import ToTensor\n",
        "from collections import defaultdict "
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZwnYpBX_ZBh"
      },
      "source": [
        "## Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BD8JKf16b0EH"
      },
      "source": [
        "class CustomImageDataset( Dataset ):\n",
        "\n",
        "    def __init__( self, annotations_file, img_dir, mask_dir,  transform=None,\n",
        "            target_transform=None ):\n",
        "        self.img_labels = pd.read_csv( annotations_file ) # the whole dataframe\n",
        "        self.img_dir = img_dir # directory for the images \n",
        "        self.mask_dir = mask_dir # directory for the masks \n",
        "\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__( self ):\n",
        "        return len( self.img_labels )\n",
        "\n",
        "    def __getitem__( self, idx ):\n",
        "        img_path = os.path.join( self.img_dir, self.img_labels.iloc[ idx, 1 ] )\n",
        "        mask_path = os.path.join( self.mask_dir, self.img_labels.iloc[ idx, 1 ]) \n",
        "\n",
        "        image = read_image( img_path )\n",
        "        mask = read_image( mask_path )\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform( image )\n",
        "        \n",
        "        if self.target_transform:\n",
        "            mask = self.target_transform( mask )\n",
        "        \n",
        "        sample = {'image': image, \"mask\": mask } \n",
        "        return sample\n",
        "\n",
        "\n",
        "    # def organize_files():\n",
        "    #     file_dir = self.img_dir # directory to all images \n",
        "    #     file_path = Path( file_dir ) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUnfKDD3qLEx"
      },
      "source": [
        "def organize_files(): \n",
        "    file_dir = 'drive/MyDrive/projects/datasets/comma10k/imgs'\n",
        "    file_path = Path( file_dir )\n",
        "    fn_arr = [] \n",
        "    \n",
        "    # loop through files \n",
        "    for fn in file_path.iterdir():\n",
        "        fn_arr.insert( 0, fn.name )\n",
        "    \n",
        "    \n",
        "    df_dic = {'filename': fn_arr, \n",
        "            'maskpath': 'drive/MyDrive/projects/datasets/comma10k/masks', \n",
        "            'imagepath': 'drive/MyDrive/projects/datasets/comma10k/imgs'}\n",
        "    \n",
        "    df = pd.DataFrame( df_dic ) # convert into dataframe\n",
        "    filename = 'filesData.csv'\n",
        "    df.to_csv( filename ) \n",
        "\n",
        "    return df "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoBDcB2TT63e"
      },
      "source": [
        "df = organize_files() # create dataframe, save csv  \n",
        "dframe = pd.read_csv( 'filesData.csv' ) # open csv "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVwJUUnEvVOf",
        "outputId": "da16f51d-3d30-48dd-f06a-98ac2d9a0508"
      },
      "source": [
        "# create a dataset \n",
        "dataset = CustomImageDataset('filesData.csv', \n",
        "                             'drive/MyDrive/projects/datasets/comma10k/imgs', \n",
        "                             'drive/MyDrive/projects/datasets/comma10k/masks' )\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.CustomImageDataset at 0x7f1356bc0f50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Uj41Q7t5avr",
        "outputId": "da358300-c373-4bf4-d0ac-37638824f81e"
      },
      "source": [
        "dataset[3]['image']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0,  1,  0,  ..., 10, 24, 24],\n",
              "         [ 0,  0,  0,  ...,  9, 24, 24],\n",
              "         [ 0,  0,  0,  ..., 13, 29, 29],\n",
              "         ...,\n",
              "         [25, 27, 26,  ..., 37, 32, 38],\n",
              "         [ 2,  4, 10,  ..., 41, 40, 43],\n",
              "         [ 1,  2,  6,  ..., 42, 42, 43]],\n",
              "\n",
              "        [[ 0,  0,  6,  ..., 25, 19, 19],\n",
              "         [ 0,  0,  4,  ..., 24, 19, 19],\n",
              "         [ 0,  0,  2,  ..., 21, 18, 18],\n",
              "         ...,\n",
              "         [ 0,  0,  5,  ...,  4,  2,  8],\n",
              "         [ 0,  2, 11,  ...,  0,  0,  1],\n",
              "         [ 0,  0,  7,  ...,  0,  0,  1]],\n",
              "\n",
              "        [[11, 19, 19,  ..., 11,  5,  5],\n",
              "         [10, 16, 17,  ..., 10,  5,  5],\n",
              "         [10, 14, 13,  ...,  9,  3,  3],\n",
              "         ...,\n",
              "         [ 0,  0,  0,  ...,  4,  3,  9],\n",
              "         [ 0,  0,  3,  ..., 34, 29, 32],\n",
              "         [ 0,  0,  0,  ..., 35, 31, 32]]], dtype=torch.uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9F15Xv0w9iwS",
        "outputId": "b8846fae-c4b5-4aa1-ccbb-8beb5bdfb3c3"
      },
      "source": [
        "dataset[3]['image'].size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 874, 1164])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8cEyJ2HAywj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbacdce2-3c65-4a3d-fd31-84bea267b9c6"
      },
      "source": [
        "x = torch.tensor( [[1,2], [2,3], [5,6]])\n",
        "\n",
        "x.size()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydaW07B3TQ7u"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhHEDZSmS5Nu"
      },
      "source": [
        "def double_conv( in_c, out_c ): \n",
        "    # input channel, output channel \n",
        "    conv = nn.Sequential( \n",
        "        nn.Conv2d( in_c, out_c, kernel_size=3), \n",
        "        nn.ReLU( inplace=True ), # what does this do?\n",
        "        \n",
        "        nn.Conv2d( out_c, out_c, kernel_size=3), \n",
        "        nn.ReLU( inplace=True ) # what does this do?\n",
        "    )\n",
        "    return conv\n",
        "\n",
        "def crop_img( tensor, target_tensor ): \n",
        "    target_size = target_tensor.size()[2]\n",
        "    tensor_size = tensor.size()[2]\n",
        "    delta = tensor_size - target_size\n",
        "    delta = delta // 2 \n",
        "    return tensor[ :,:,delta:tensor_size-delta, delta:tensor_size-delta ]\n",
        "\n",
        "\n",
        "class UNet( nn.Module ): \n",
        "\n",
        "    def __init__( self ): \n",
        "        super( UNet, self ).__init__() # what does this do?\n",
        "\n",
        "        self.max_pool_2x2 = nn.MaxPool2d( kernel_size=2, stride=2 ) # define this 1x, use it multiple times \n",
        "    \n",
        "        self.down_conv_1 = double_conv( 1, 64)\n",
        "        self.down_conv_2 = double_conv( 64, 128)\n",
        "        self.down_conv_3 = double_conv( 128, 256)\n",
        "        self.down_conv_4 = double_conv( 256, 512)\n",
        "        self.down_conv_5 = double_conv( 512, 1024)\n",
        "\n",
        "        self.up_trans_1 = nn.ConvTranspose2d( \n",
        "            in_channels = 1024,\n",
        "            out_channels = 512, \n",
        "            kernel_size = 2,\n",
        "            stride = 2 \n",
        "        )\n",
        "        self.up_conv_1 = double_conv( 1024, 512 )\n",
        "             \n",
        "        self.up_trans_2 = nn.ConvTranspose2d( \n",
        "            in_channels = 512,\n",
        "            out_channels = 256, \n",
        "            kernel_size = 2,\n",
        "            stride = 2 \n",
        "        )\n",
        "        self.up_conv_2 = double_conv( 512, 256 )\n",
        "        \n",
        "        self.up_trans_3 = nn.ConvTranspose2d( \n",
        "            in_channels = 256,\n",
        "            out_channels = 128, \n",
        "            kernel_size = 2,\n",
        "            stride = 2 \n",
        "        )\n",
        "        self.up_conv_3 = double_conv( 256, 128 )\n",
        "        \n",
        "        self.up_trans_4 = nn.ConvTranspose2d( \n",
        "            in_channels = 128,\n",
        "            out_channels = 64, \n",
        "            kernel_size = 2,\n",
        "            stride = 2 \n",
        "        )\n",
        "        self.up_conv_4 = double_conv( 128, 64 )\n",
        "\n",
        "\n",
        "        self.out = nn.Conv2( \n",
        "            in_channels = 64, \n",
        "            out_channels = 5, # increase out channels based on how many classes youwant to segement \n",
        "            kernel_size = 1 # 2d conv, w kernel size of 1\n",
        "        )\n",
        "\n",
        "    def forward( self, image ):\n",
        "        # bs, c, h, w --> batch size, channel, height, width \n",
        "        # Encoder file \n",
        "        x1 = self.down_conv_1( image ) # \n",
        "        x2 = self.max_pool_2x2( x1 )\n",
        "        x3 = self.down_conv_2( x2 ) # \n",
        "        x4 = self.max_pool_2x2( x3 ) \n",
        "        x5 = self.down_conv_3( x4 ) # \n",
        "        x6 = self.max_pool_2x2( x5 ) \n",
        "        x7 = self.down_conv_4( x6 ) # \n",
        "        x8 = self.max_pool_2x2( x7 ) \n",
        "        x9 = self.down_conv_5( x8 ) # \n",
        "        print( x9.size() )\n",
        "\n",
        "        # decoder part \n",
        "        x =  self.up_trans_1( x9 ) \n",
        "        y = crop_img( x7, x ) \n",
        "        x = self.up_conv_1( torch.cat( [x, y], 1) )\n",
        "        \n",
        "        x =  self.up_trans_2( x ) \n",
        "        y = crop_img( x5, x ) \n",
        "        x = self.up_conv_2( torch.cat( [x, y], 1) )\n",
        "\n",
        "        x =  self.up_trans_3( x ) \n",
        "        y = crop_img( x3, x ) \n",
        "        x = self.up_conv_3( torch.cat( [x, y], 1) )\n",
        "\n",
        "        x =  self.up_trans_4( x9 ) \n",
        "        y = crop_img( x1, x ) \n",
        "        x = self.up_conv_4( torch.cat( [x, y], 1) )\n",
        "\n",
        "        x = self.out( x ) \n",
        "        print( x.size() ) \n",
        "        return x \n",
        "\n",
        "\n",
        "if __name__ == \"__main__\": \n",
        "    image = torch.rand( ( 1, 1, 572, 572 )) \n",
        "    model = UNet() \n",
        "\n",
        "    print( model( image ) )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}